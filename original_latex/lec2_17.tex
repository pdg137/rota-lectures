{\Large 18.313 Lecture 7, Wednesday, February 17, 1999}\newline
{\large Lecture by Prof. G.-C. Rota}\newline
Transcribed by Vladislav Gabovich (vyg@mit.edu)\newline

\noindent {\bf Random Variables} (continued):

Let us continue with the topic of random variables, which is relevant
to our study of the Bernoulli process.
We saw in previous lectures that in the Bernoulli process, we have
several notable random variables.
For example, let us define
\begin{equation}
X_{n} = \mbox{ the outcome of the n-th toss.}
\end{equation}
Also,
\begin{eqnarray*}
P(X_{n}=1) &=& p  \\
P(X_{n}=0) &=& q=1-p
\end{eqnarray*}
The random variables in the sequence $X_{1},X_{2},X_{3},\ldots$ are
independent, that is, every finite subset of them is independent.
(An elegant way to define the Bernoulli process is through a
sequence of random variables).
In real life, sample spaces are more often defined by random variables
and their distributions, than by sample points, events and their
probability. In quantum probability, for example, one is forced to
study random variables, since there is no "point" space to look at. 

\noindent (cont'd) {\bf The Bernoulli Process }

Consider the sequence
\begin{equation}
T_{1},T_{2},T_{3}, \ldots
\end{equation}
where $T_{i}$ denotes the gap, in tosses, between successive "heads".
Last time, we argued convincingly that these random variables are
independent, and that their probability distributions are given by
\begin{equation}
P(T_{1}=n)=q^{n-1}p
\end{equation}
Then we considered the random variable
\begin{equation}
S_{n}= X_{1}+X_{2}+X_{3}+\ldots+X_{n},
\end{equation}
the number of successes (heads) in the first n tosses.
This random variable is given by
\begin{equation}
P(S_{n}=k)= {n \choose k} \cdot p^{k}q^{n-k}
\end{equation}
This is our "bread-and-butter" probability - don't ever forget it!

Let's talk about expectations now.
What is the expectation of $X_{n}$, $E(X_{n})$ ?
By definition, the scriptures say, it is:
\begin{equation}
E(X_{n}) = \sum_{j} j \cdot P(X_{n}=j)
\end{equation}
Since $X_{n}$ takes only the values \{ $0,1$ \}, this sum reduces to
the first two terms:
\begin{equation}
E(X_{n}) = 0 \cdot P(X_{n}=0) + 1 \cdot P(X_{n}=1) = p
\end{equation}
Next, let's compute $S_{n}$. The scriptures say that this must
be computed in terms of the probability distribution,
\begin{eqnarray*}
E(S_{n}) &=& \sum_{k=0}^{n}k\cdot P(S_{n}=k) \\
&=& \sum_{k=0}^{n}k{n \choose k} \cdot p^{k}q^{n-k} = mess
\end{eqnarray*}
There must be a simpler way to do this. We would go into a 
calculation like the one above only if we were "backed to the wall".
We'll use the God-given theorem : observe, that
\begin{equation}
E(S_{n}) = E(X_{1}) + E(X_{2}) + E(X_{3}) + \ldots + E(X_{n})  
\end{equation}
But, $X_{1} \ldots X_{n}$ are identically distributed, and hence
have the same expectation,
\begin{equation}
E(S_{n}) = E(X_{1}) = n \cdot p  
\end{equation}
And the horrendous sum is just that, $n \cdot p$ !
Now $E(T_{1})$ - there is a trick to do it in one step, 
$ = \frac{1}{p}$ (Why?)
The bigger $p$ is, the shorter the wait.\\
Let's now consider
\begin{equation}
W_{k} = \mbox{the waiting time for the k-th head,}
\end{equation}
another important random variable.
Clearly,
\begin{equation}
W_{k} = T_{1}+T_{2}+T_{3}+ \ldots + T_{k}
\end{equation}
We could compute the distribution of $W_{k}$ the hard way,
using the fact that $T_{i}$s are independent, but instead,
notice that the event $P(W_{k})$ means

1. "1" at the n-th toss \\
2. Pick $k-1$ "1"'s from first $n-k$ tosses

Therefore, we write
\begin{equation}
P(W_{k}=n)={{n-k}\choose {k-1}}p^{k}q^{n-k}
\end{equation}
This topic inspires some tough questions, ones of great
relevance in practice. For example, to test the randomness
of a sequence, we can try to check probabilities of different
patterns.

\noindent {\bf The Borel-Cantelli lemma}

This lemma is best understood from examples.
Consider a pattern - a run of 17 consecutive heads ("1"s).
We ask the following question:
Suppose $p<1$. What is the probability that this run will
ever occur? \\
Let $B =$ the event that a run of 17 heads will occur.
We want to show that $P(B) =1$.
(Intuitively this is fine; but let us stop handwaving.
our proof will be rigorous, though cute).\\
Let us consider other events:
\begin{eqnarray*}
B_{1} &=& \mbox{event that tosses 1-17  are "1"s} \\
B_{2} &=& \mbox{event that tosses 18-34 are "1"s,}
\end{eqnarray*}
etc, a sequence \{ $B_{n}$ \}. 
Of course, a run can occur anywhere, but our argument
will be made on the basis of these cases.
Observe, that $B_{1},B_{2}, \ldots$ are independent.
Since B is the event that a run of 17 "1"s {\bf} ever occurs,
\begin{equation}
B \supseteq B_{1} \cup B_{1} \cup B_{1} \cup \ldots
\end{equation}
which means
\begin{equation}
B \subseteq B_{1}^{c} \cup B_{1}^{c} \cup B_{1}^{c} \cup \ldots
\end{equation}
Since $B_{i}$'s are independent, $B_{i}^{c}$'s are independent as well.
Hence, we obtain,
\begin{eqnarray*}
P(B^{c}) & \leq & P( B_{1}^{c} \cup B_{1}^{c} \cup \ldots \cup B_{k}^{c}
\mbox{ } \forall k \\
& = & P(B_{1}^{c})P(B_{2}^{c}) \ldots P(B_{k}^{c})
\end{eqnarray*}
Now,
\begin{equation}
P(B_{i}^{c}) = 1-P(B_{i}) = 1-p^{17}
\end{equation}
Of course, $P(B_{2})=P(B_{1})$; therefore
\begin{equation}
P(B^{c}) = 1-P(B) = (1-p^{17})^k
\end{equation}
As k $\rightarrow \infty$, $P(B^{c}) \rightarrow 0$,
hence
\begin{equation}
P(B) = 0
\end{equation}
We have shown, beyond a reasonable doubt, that $P(\mbox{run of 17}) = 1$.

By the same reasoning, the Borel-Cantelli lemma applies to any 
pattern whatsoever, and we have
\begin{equation}
P(\mbox{\bf any pattern}=1)
\end{equation}
Not quite. What we really proved is more - not only does a run 
(of 17 heads) sometimes occur, but it occurs {\bf infinitely} 
many times. For, suppose the pattern stops occuring after some
finite step $N$. This immediately contradicts what we just proved!
Treating N as our starting step, since the pattern doesn't know
that it already occured, it must occur again, for the "first time".
Applied to any finite step, and for any finite pattern, this argument 
implies that:
\begin{center}
\bf Every finite pattern occurs infinitely many times with probability
1
\end{center}
In other words, typing at random, a monkey will type the Bible 
infinitely many times! \\
Question: Why doesn't this argument apply to infinite patterns? \\
Answer:   Because if $l=\infty$, $P(B^{c}) \leq (1-p^{l})^{k} \neq 0$

Now we will proceed to fresh sample spaces and random variables,
sampling with and without replacement - "bread-and-butter"
probability.

\noindent {\bf Sampling with Replacement}

Imagine a urn of balls, marked $1 \ldots n$, and let $c_{i}$ denote the
number of balls marked $i$ in the urn, $1 \leq i \leq n$.
We will extract a ball at random from the urn, examine it, and put it
back. We will define the sample space as 
\begin{equation}
\Omega = \{ w = (w_{1},w_{2}, \ldots,w_{i},\ldots); 1 \leq i \leq n \}
\end{equation}
The probabilities in this example are obvious.\\
Let's define 
\begin{equation}
X_{n} = \mbox{ the result of the n-th sample.}
\end{equation}
Then
\begin{equation}
P(X_{i}=1) = \frac {c_{1}}{c_{1}+c_{2}+\ldots+c_{n}}
\end{equation}
The random variables $X_{1}, X_{2}, \ldots$ are independent. Events are the
values of random variables. The sample space of sampling with
replacement is very similar to coin tossing, that is, tossing of an n-sided
die, and therefore the questions are similar. 

We've been talking about a sequence of independent random variables 
which are identically distributed. In the next lecture, professor 
S.Billey will talk about random walks.

\noindent {\bf Sampling without Replacement}

The process of picking out balls from the urn is the same, except
we throw out each ball we pick out, instead of putting it back into
the urn.
\begin{equation}
\Omega = \{w = (w_{1},w_{2}, \ldots, w_{c_{1}+\ldots+c_{n}} \}
\end{equation}
Again, define,
\begin{equation}
X_{n} = \mbox{result of the n-th extraction.}
\end{equation}
Clearly,
\begin{equation}
P(X_{1}=i) = \frac {c_{i}} {c_{1}+\ldots+c_{n}}
\end{equation}
I claim,
\begin{equation}
P(X_{n}=i) = \frac {c_{i}} {c_{1}+\ldots+c_{n}} = P(X_{1}=i) 
\end{equation}

{\it Proof by the principle of sufficient reasoning:} extract all the balls
and permute them; balls do not think, so the probability distribution
is the same, whether we look at the 1st or n-th extraction.\\
$\Longrightarrow$ \\
$X_{1},X_{2},\ldots$ are not independent, but identically distributed.

How do we get a joint distribution of random variables? \\
Suppose we want to compute $P((X_{3}=i) \cap (X_{n}=j))$. Again, by
the principle of sufficient reasoning, we argue that since the balls 
do not think and the distributions are the same, we can freely permute
the indices:
\begin{eqnarray*}
P((X_{3}=i) \cap (X_{n}=j)) & = & P((X_{1}=i) \cap (X_{2}=j)) \\
& = &
\left\{ \begin {array}{rcl}
\frac{c_{i}c_{j}}{(c_{1}+c_{2}+\ldots+c_{n})(c_{1}+c_{2}+\ldots+c_{n-1})}
& if & i \not = j\\
\frac{c_{i}(c_{i}-1)}{(c_{1}+c_{2}+\ldots+c_{n})(c_{1}+c_{2}+\ldots+c_{n-1})}
& if & i = j
\end{array} \right\}
\end{eqnarray*}
Question: How to express precisely the fact that it doesn't matter
which is which, that we can permute the extractions?



{\Large 18.313 Lecture 9, Wednesday, February 24, 1999}\newline
{\large Lecture by Prof. G.-C. Rota}\\
Transcribed by Bilge Demirk\"oz (bilge@mit.edu) and Vladislav Gabovich (vyg@mit.edu)\\

\noindent {\bf Conditional Probability} (continued):

Last time, we defined the conditional probability of an event given
that another event happens:
\begin{equation}
P(A|B)= \frac {P(A \cap B)} {P(B)}
\end{equation}
As we observed last time, this is sometimes written $P_{B}(A)$, 
where A is the variable event: as A varies, its probability is
defined on a fixed sample space (B).
Of course, strictly speaking, every probability is conditional.

Last time, we discussed the following two fundamental properties
of conditional probability:

\noindent {\bf I. Law of Alternatives}:

Here we will be talking about a partition of the sample space
into "blocks" of probabilities - a union of disjoint blocks
that covers $\Omega$.

For example, given an integer random variable $X$, we have
\begin{equation}
P(A)= \sum_{n} P(A|X=n)\cdot P(X=n)
\end{equation}
It is possible that $P(X=n)=0$ for some $n$. Events $(X=n)$ for which
$P(X=n)>0$ form a partition of $\Omega$.

\noindent {\bf II. Law of Successive Probabilities} (continued):

This is the second fundamental computation where one applies 
conditional probability. As we have started to discuss last time,
\begin{eqnarray*}
P(A \cap B) &=& P(A)\cdot P(B|A) = P(A) \cdot \frac{P(A \cap B)}{P(A)}\\ 
P(B_1\cap B_2 \cap B_3) &=& P(B_1) \cdot P(B_2|B_1) \cdot P(B_3|B_1 \cap B_2)\\ 
&=& P(B_1) \cdot \frac{P(B_1 \cap B_2)}{P(B_2)} \cdot \frac{P(B_1 \cap B_2 \cap
B_3)}{P(B_1 \cap B_2)}\\ 
P(B_1 \cap B_2 \cap \ldots \cap B_n) &=& P(B_1) \cdot P(B_2|B_1) \cdot
P(B_3|B_1 \cap B_2) \ldots P(B_n | B_1 \cap B_2 \cap \ldots \cap B_{n-1})\\
\end{eqnarray*}
The verification of this law is immediate, as we have previously
sketched.\\
Note, that there exists a $3^{rd}$ law of some importance, but these are 
our "bread-and-butter" laws.

\noindent {\bf Examples: Sampling without Replacement}:

We have an urn with $r$ red balls and $b$ black balls. Then we have a
sequence of extraction $X_1$, $X_2$,\ldots,$X_{r+b}$. Previosly, we
have discussed this rather informally, in fact, you should have asked
yourself what was the sample space, and how probability was defined.
Now let's do it rigorously, using conditional probability.

\begin{eqnarray*}
P(X_1=1) &=& \frac{r}{r+b}\\
P(X_1=0) &=& \frac{b}{r+b}\\
P((X_1=1) \cap (X_2=0)) &=& P(X_1=1) \cdot P(X_2=0|X_1=1)\\
P(X_2=0|X_1=1) &=& \frac{b}{r+b-1}\\
P((X_1=1) \cap (X_2=0)) &=& \frac{r\cdot b}{(r+b)(r+b-1)}\\
\end{eqnarray*}
{\it Conclusion:} the way we define probability on sample space without
replacement is done in terms of conditional probability.
Let's do more. Similarly,
\begin{eqnarray*}
P((X_1=1) \cap (X_2=0) \cap (X_3=1)) &=& P(X_1=1) \cdot P(X_2=0|X_1=1)
\cdot P(X_3=1|(X_1=1) \cap (X_2=0))\\
&=& \frac{r}{r+b} \cdot \frac{b}{r+b-1} \cdot \frac{r-1}{r+b-2}
\end{eqnarray*}

Let's generalize this result. We will try to get a formula for any
sequence of extractions. More generally,
\begin{equation}
P((X_{1}=i_{1}) \cap (X_{2}=i_{2}) \cap \ldots \cap (X_{n}=i_{n})) = ?
\end{equation}
Notice how you go down by one in the numerator according to if its
red or black. How do we express this? This probability is computed
by the law of successive conditioning. It depends only on how many 
$i_1$, $i_2$,\ldots,$i_n$'s equal $1$. 
Lets say $i$ of them equals $1$ and $j$ of them equals $0$.
\begin{eqnarray*}
P((X_1=i_1) \cap (X_2=i_2) \cap \ldots \cap (X_n=i_n)) &=& 
\frac{r(r-1)\ldots(r-i+1)b(b-1)\ldots(b-j+1)}{r+b)(r+b-1)\ldots(r+b-i-j+1)}\\
&=& \frac{(r)_j (b)_j}{(r+b)_(i+j)}
\end{eqnarray*}
(by preceding reasoning, we don't know when the number of red/black
balls clicks down by one, but multiplication is commutative, so
we don't care!).

Before we philosophize on this result, let's compute something else.
What we have just computed is the probability of a sequence of 
$i$ 1's and $j$ 0's. It is not the probability of $i$ blacks and $j$ reds.
Let us now compute the other probability. Let $i+j=n$
Let A be the event that after extracting $n$ balls, I have $i$ red
balls and $j$ black balls. Then there are ${{r+b}\choose {n}}$ ways of
extracting the balls. So we get:
\begin{eqnarray*}
P(x=i) &=& \frac{{{r}\choose{i}} {{b}\choose{j}}}{{{r+b}\choose{n}}}=
\frac{{{r}\choose{i}} {{b}\choose{n-i}}}{{{r+b}\choose{n}}}
=\frac{\frac{(r)_i}{i!} \frac{(b)_j}{j!}}{\frac{(r+b)_n}{n!}}\\
&=& \frac{n!}{i! j!} \frac{(r)_i (b)_j}{(r+b)_{i+j}}=
{{n}\choose{j}} \frac{(r)_i (b)_j}{(r+b)_{i+j}}
\end{eqnarray*}
This is called the {\it Hyper-geometric distribution}.

But this is simply the ways of distributing $n$ balls in $i$ places.
We didn't have to do it indirectly.
Now lets solve another problem. 
We have an urn with $n$ number of colors and so the sequence of 
extractions is $X_1$,$X_2$,\ldots,$X_{c_1+c_2+\ldots+c_n-1}$. Then,
\begin{eqnarray*}
P((X_1=i) \cap (X_2=j)) &=& P(X_1=i) \cdot P(X_2=j|X_1=i)\\ 
\mbox{if } i\not=j \mbox{ then} &=& \frac{c_i}{c_1+c_2+\ldots+c_n} \cdot
\frac{c_j}{c_1+c_2+\ldots+c_n-1}\\
\mbox{if } i=j \mbox{ then} &=& \frac{c_i}{c_1+c_2+\ldots+c_n} \cdot
\frac{c_i-1}{c_1+c_2+\ldots+c_n-1}\\ 
\end{eqnarray*}

Let's solve for the general case, $P((X_1=i_1) \cap (X_2=i_2) \cap \ldots
\cap (X_k=i_k))$.  If among $i_1$, $i_2$,\ldots,$i_k$ there are $j_1$
equal to $1$, $j_2$ equal to $2$ and $j_n$ equal to $n$, then,  
\begin{equation}
P((X_1=i_1) \cap (X_2=i_2) \cap \ldots \cap (X_k=i_k))=
\frac{(c_1)_{j_1} (c_2)_{j_2} \ldots
(c_n)_{j_n}}{(c_1+c_2+\ldots+c_n)_{k}}
\end{equation}

In a sample space of $k$ marbles, the probability of finding $j_1$
balls marked $1$, $j_2$ balls marked $2$ and $j_n$ balls marked $n$
where $j_1+j_2+\ldots+j_n$ is equal to $k$:
\begin{eqnarray*}
\frac{{{c_1}\choose{j_1}} {{c_2}\choose{j_2}} \ldots
{{c_n}\choose{j_n}}}{{{c_1+c_2+\ldots+c_n}\choose{k}}} &=& 
{{k}\choose{j_1, j_2,\ldots,j_n}} \frac{(c_1)_{j_1} (c_2)_{j_2} \ldots
(c_n)_{j_n}}{(c_1+c_2+\ldots+c_n)_{k}}\\
P(X_2=j) &=& \sum_{i} P(X_2=j|X_1=i) \cdot P(X_1=i)\\
\mbox{but } P(X_2=j|X_1=i) &=& (\frac{c_i-1}{c_1+c_2+\ldots+c_n-1})+ 
(\sum_{j\not=i} \frac{c_j}{c_1+c_2+\ldots+c_n-1})\\
\mbox{and }P(X_1=i) &=& \frac{c_i}{c_1+c_2+\ldots+c_n}\\
\end{eqnarray*}
After a lot of algebra we have -- if you do not believe me you can
calculate it for yourselves:
\begin{equation}
P(X_2=j)= \frac{c_{j}}{c_1+c_2+\ldots+c_n}\\
\end{equation}

But there is an easier way to do this. 
\begin{eqnarray*}
P(X_2=i) &=& P(X_1=i)\\
P((X_5=j) \cap (X_2=i)) &=& P((X_1=j) \cap (X_2 =i))
\end{eqnarray*}
This is a very important fact as we had mentioned before.
We can compute all joint distributions very easily because they
are symmetric.

{\bf Definition}:

A sequence of $X_1$, $X_2$,\ldots of integer random variables is
{\bf exchangeable} when all \\
$P((X_i) \cap \ldots \cap (X_k=i))$ depend
only on how many $i_1$, $i_2$,\ldots,$i_n$ equal
\ldots,$-1$,$0$,$1$,$2$,\ldots.

For example in Maxwell-Boltzmann Statistics,
$\Theta_1$,$\Theta_2$,\ldots$\Theta_n$ are exchangeable.

If you have an infinite sequence of events, it is exchangeable if any
subset is exchangeable.

A sequence $X_1$, $X_2$,\ldots of independent identically distributed
random variables is ``a fotiori'' also exchangeable.

